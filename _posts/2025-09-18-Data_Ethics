#Hello!
Welcome to my very first post on this blog! 

I belive this is going to be a bit shallow as i cover the material, but we will get better with time!
I am currently reading the third chapter(of the book FasAI) and I am almost through with it. 
To remember and solidfy my thoughts i am going to make a couple of notes here! Hopefully you find them helpful and if not then no issues (look out for better things in the future!)

##Introduction

The problem with the datasets that our NN are trained on are that they have data issues (such as biased data, misinformated data etc)
and since the boom of Deep Learning was so recent that it is imperative that we look at the results that these NN are giving and question ourselves
wheather this is something we agree with or is something we need to look at from a better ethical and moral standpoint.

Some examples of Data ethics :

1. "After implementation of the algorithm in Arkansas,
hundreds of people (many with severe disabilities) had their healthcare drastically cut." - Chap 3

This speaks about the time an algorithm was put in place of giving and checking if the given healtcare was aqeuate or more and a LOT of people were left stranded with their 
healthcare being drastically cut due to biases and issues in the underlying dataset.
(I personally believe that there needs to be a good amount of human oversight with stuff like this)

2. " Feedback loops can occur when your model is controlling the next round of data you get.
The data that is returned quickly becomes *flawed by the software itself.*

Youtube shows a **lot** of content to people in the world and the fact that it is motivated to optimize watch time might cause people to be fed
the same kind of data at a much amplified pace to keep them on and this can skew their mindest ever more ( since they are not showed things from another viewpoint this might reinforce their view points)

###Why might this matter?

The thing is that it is going to matter little if your viewpoint is different to someone who is in a senior position to you if push come to shove.
BUT change start from one person - so even if this chapter and notes are almost never used it is better to be preapred than to not be.

We humans have a moral obligation to help the people around us and maybe asking ourselves certain question when we are working on things that might have large 
societal impact, will be better for all of us in the long run.

I understand that this might feel like putting too much importance onto singlar than an organization (esp if your voice is chump change in front of a multi-billion dollar company)
but doing what we can in our power and inspiring people around us to do the same is something we should all strive to do.

"Sometimes, the right response to being asked to do a piece of work is to just say “no.” 
Often, however, the response we hear is, “If I don’t do it, someone else will.” But consider this: if you’ve been picked for the job, you’re the best person they’ve found to do it—so if you don’t do it, the best person isn’t working on that project. 
If the first five people they ask all say no too, so much the better!" " - i thought this should be written here from the book beacuse it is v apt.


##Topics in Data Ethics 

There are certain topics in Data ethics that stand out more than others so lets look at them

###Recourse and Accountability

Deep Learning has so many systems that need to work together like - data collection, data cleaning, paramteer selection, etc.
No doubt that a **lot** of people are involved, so when something goes wrong it is very easy to say that it wasn't me but that department, or that person, or that process, or because of these assumptions, etc.
As you can see the way to deflect blame is endless, so we need a way to audit and make sure that we know when and how something went wrong - to correct and take responsibility so we can be better for the future.

In the current system (In accordance to when the book was written) the machanism for this is incredibly slow and opaque.

###Feedback loops

I think this is best described by the book by this line - "An algorithm can interact with its environment to create a feedback loop, making predictions that reinforce actions taken in the real world,
hich lead to predictions even more pronounced in the same direction"

###Bias

The bias we are talking are the ones we see in social science. There are 4 types of bias highlighted here.

####Historical Bias

Historical bias comes from the fact that people are biased, processes are biased and that people are biased.

An example given in the book that summarises this issue properly is :

"The COMPAS algorithm, widely used for sentencing and bail decisions in the US,
is an example of an important algorithm that, when tested by ProPublica, showed clear racial bias in practice"

The datasets can have any kind of bias - a bias on the basis of medical data, sales, political and so on.

The fact is that the bias that we have ( and the one that gets programmed into these datasets) are so pervasive in that fact that most of the times
we dont even know that we operate with a certain bias in mind.

One of the MIT researchers, Joy Buolamwini, warned: “We have entered the age of automation overconfident yet underprepared.
If we fail to make ethical and inclusive artificial intelligence, we risk losing gains made in civil rights and gender equity under the
guise of machine neutrality.”

--Note : The latter half of the notes are lost due to SSD failure but ill definetely come back to update these ones - for now onto chapter 4!






